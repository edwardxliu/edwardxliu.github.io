{"posts":[{"title":"PLAY RTSP STREAM FROM WEB CAMERA ON HOLOLENS 2","content":"Development Experience Sharing We are trying to play some video streams from web cameras to the HoloLens 2 these days, and we think it's easy but it's actually not THAT easy. So we mark down what we've done. Firstly, we followed the guide here to play some videos on Hololens 2 with the Unity component of Video Player. It is quite easy and the result is also good. We also tried to play some web videos with URLs by the Video Player, and it performed also well. Howerver, it can not play stream in **RTSP ** protocol which is used by our web cameras. So we tried another player AVPro Video with some guides here. According to here, it actually supports RTSP streaming protocol. Howerver, it still can't play our web cam stream with some errors showed here. Someone discussed the similar issue at here and it says that the AVPro Video can play RTSP stream with the url format like rtsp://77.208.177.183:55556/live.sdp, while our url is in the format of rtsp://username:password@url. Since we don't know how to remove the &quot;username:password&quot; part in the url, we decide to transfer the original RTSP stream from the web camera into a new one but without authorization. The RTSP server we build is rtsp-simple-server and we use FFMPEG to pull the stream from the web camera and push it to the server. Finally, the player shows the video stream we want, but we still don't figure out how to play the stream with authorization. ","link":"https://edwardxliu.github.io/post/play-rtsp-stream-from-web-camera-on-hololens-2/"},{"title":"REAL-TIME OBJECT DETECTION ON HOLOLENS 2","content":"Development Experience Sharing We've achieved 30fps real time object detection on HoloLens 2 with 60fps rendering rate. The final result and performance are presented in below video. Since this is the first time we touch AR and we are not farmiliar with C# and Unity, we've tried a lot to fulfill the goal. Below list includes some tips and experiences which may be helpful. The official tutorial is for HoloLens 1 and we don't know how to adapt the code to be performed on HoloLens 2. Also, we don't know how to develop and deploy AI algorithms and directly execute them on HoloLens 2 since it doesn't have NVIDIA gpus (CUDA excluded) and its cpu is ARM-architectured. So we decide to make the AI algorithm running on a powerful external device and let the HoloLens to handle video capture and display the location results of objects predicted by AI algorithm. Therefore, establishing a robust and reliable communication connection between the HoloLens and the external device is necessary and important. The first way we tried is to convert each video frame captured by the HoloLens camera to a bitmap and then send the bitmap to our PC with NVIDIA gpus. A PC program receives the bitmap then predicts the names and locations of objects in that bitmap and sends those information back to HoloLens. However, the total running speed of this way is really bad. One reason we think is due to the waiting time of the information sending and receiving process on HoloLens. The other reason is that we use TCP as the transferring protocol because the correctness of each pixel in a video frame plays an important role on the prediction process of the AI model. We use YOLO as the detection algorithm cause it's fast. We tried to make the sending and receiving process asynchronous on the HoloLens, but the performance is not improved significantly. The official tutorial about the fundamentals of HoloLens 2 development is really important. It guides us on how to establish, configure, develop, deploy and debug a Unity project to the HoloLens. Other guides such as link are also helpful especially in how to set the spatial awareness mesh invisible. We tried to follow the suggestion here in order to increase the performance of the streaming process (e.g. using MixedReality-WebRTC) on HoloLens. However, the technology is a bit complicated for us. If you are interested in WebRTC, the building and deploying process can be refered here, and the signal transferring is here. We tried to use multi-threading technique to improve the performance. You can refer to here. We tried to use async producer/consumer queue design pattern to improve the performance of the streaming process. You can refer to here, here, here and the official link is here. We finally find out that the HoloLens actually has a live video stream. Thus, there is no need to send video stream to the PC anymore. Also, because of this, the bitmap conversion process and the corresponding TCP sending process are not required. Although UDP is not reliable and sometimes information is lost, we think it's informative enough to transfer the prediction results. This is because both YOLO and UDP are fast, one or two consecutive prediction-result loss won't effectively affect the rendering rate on HoloLens. It is quite important to know that the maximum rendering rate of Hololens 2 is 60 fps, which means a simple Unity program that has some basic Mixed Reality functions or draws some simple holograms will run at a maximum of 60 fps on Hololens 2. You may refer to the official links here and here. It is also important to know that the camera frame rate of HoloLens 2 is 30 fps. Some people had discussed it here. If you don't make any optimization, the program on HoloLens 2 will get capped at 30 fps, which we believe is intolerable. Therefore, since 60 fps is just a double of 30 fps, we let the HoloLens receives a UDP package in every two consecutive frames. This technique directly improves the rendering rate to 50-60 fps. Meanwhile, it does not bring negative impacts to the holograms generation and the real time object detection process. So far, we've throwed away the TCP video streaming process and the multi threading or asynchronous programing because the live streaming, YOLO and the mixed reality program are basically asynchronous and multi-processed. The only time consuming processes are the instantiate of the holograms and their maintenance. In terms of the instantiate, you may refer to here and here. We tried to use Dictionary class to manage the game objects created by Unity with the purpose of reducing the searching and matching time but it's not necessary. Hologram objects can also be stored in a array or a list. You may refer to here and here to decide which one is better. Since the maintenance of those objects requires a lot of searching process, you may refer to here, here and here to optimize your for loops. we failed at using &quot;Parallel.For&quot; because we think the iteration and the creation or removing process from a same array or list are not allowed. Finally, when delopying the Unity project to Hololens, choose Release instead of Debug mode in Visual Studio will significantly improve the performance of the program on HoloLens. ","link":"https://edwardxliu.github.io/post/video-test/"},{"title":"SIGMOID FUNCTION OPTIMIZATION","content":"The definition of the sigmoid function is sigmoid(x)=11+e−xsigmoid\\left ( x \\right )=\\frac{1}{1+e^{-x}} sigmoid(x)=1+e−x1​ In Python code, it would be sth. like below shows. import math def sigmoid(x): return 1 / (1 + math.exp(-x)) Because it involves a division and a exponent operation, it will cost a certain amount of computing time during the training and prediction process of a neural network. However, since we often use the output of the sigmoid function in a comparison of a given threshold T like if sigmoid(x) &gt; T: ... do something we can rewrite the sigmoid formula like below. f(x)=y=11+e−x⇒1y=1+e−x⇒1y−1=e−x⇒ln(1y−1)=−x⇒−ln(1y−1)=x\\begin{aligned} % requires amsmath; align* for no eq. number f\\left ( x \\right )&amp;=y=\\frac{1}{1+e^{-x}} \\\\ &amp;\\Rightarrow\\frac{1}{y}=1+e^{-x} \\\\ &amp;\\Rightarrow\\frac{1}{y}-1=e^{-x} \\\\ &amp;\\Rightarrow ln\\left ( \\frac{1}{y} -1 \\right )=-x \\\\ &amp;\\Rightarrow-ln\\left ( \\frac{1}{y} -1 \\right )=x \\end{aligned} f(x)​=y=1+e−x1​⇒y1​=1+e−x⇒y1​−1=e−x⇒ln(y1​−1)=−x⇒−ln(y1​−1)=x​ Now we get the inverse function of f(x)f(x)f(x). f−1(x)=−ln(1y−1)f^{-1}\\left ( x \\right )=-ln\\left ( \\frac{1}{y} -1 \\right ) f−1(x)=−ln(y1​−1) Therefore, in terms of the if statement above, we can take the inverse on the two sides of &gt;. The left side would be x, and the right side would be a desigmoid threshold −ln(1T−1)-ln\\left ( \\frac{1}{T} -1 \\right )−ln(T1​−1) which is a constant as well. So, in the end, the above if statement can be rewrited to if x &gt; desigmoid_T: ... do something This will significantly increase the performance of the if statement. ","link":"https://edwardxliu.github.io/post/sigmoid-function-optimization/"},{"title":"CODE REVIEW OF LANGUAGE TRANSLATION WITH NN.TRANSFORMER AND TORCHTEXT - PART 2","content":"Token Embedding The second part of the source code is related to token and its position embedding. We reivew the former firstly. from torch import Tensor import torch import torch.nn as nn from torch.nn import Transformer import math DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu') # helper Module to convert tensor of input indices into corresponding tensor of token embeddings class TokenEmbedding(nn.Module): def __init__(self, vocab_size: int, emb_size): super(TokenEmbedding, self).__init__() self.embedding = nn.Embedding(vocab_size, emb_size) self.emb_size = emb_size def forward(self, tokens: Tensor): return self.embedding(tokens.long()) * math.sqrt(self.emb_size) The TokenEmbedding class just includes an initialize function and a forward function where the __init__ function (aka. the constructor) is called when the class is instantiated as an object, and the forward function is called when parsing parameters directly to the object, which invokes the __call__ method because the class simply inherits the __call__ method of the nn.Module class. It is here where the forward method is called. Below test code presents an example. &gt;&gt;&gt; class Test(nn.Module): ... def __init__(self): ... super(Test, self).__init__() ... print('Test class __init__') ... def forward(self, x): ... return x + 1 ... &gt;&gt;&gt; t = Test() Test class __init__ &gt;&gt;&gt; t(1) 2 &gt;&gt;&gt; Back to the source code, the initialization of the TokenEmbedding class is related to the instantiation of nn.Embedding class and two parameters, vocab_size and emb_size which refer to the length of the token dictionary vocab_transform mentioned in here and the length of each word vector. So, each token in the dictionary will be associated with a number vector (tensor) of length equal to emb_size. The nn.embedding(num_embeddings, embedding_dim) will initialize a tensor (learnable weights) with shape (num_embeddings, embedding_dim) from N(0,1)\\mathbb{N}\\left ( 0, 1 \\right )N(0,1). We can review the values in the tensor by calling the .weight function and check if it is normally distributed by p value from scipy.stats.normaltest function as below code shows. It is more likely to be a normal distribution when p value is closer to 1. &gt;&gt;&gt; from torch import nn &gt;&gt;&gt; from scipy import stats &gt;&gt;&gt; &gt;&gt;&gt; embedding = nn.Embedding(10,512) &gt;&gt;&gt; print(embedding.weight.shape) torch.Size([10, 512]) &gt;&gt;&gt; print(embedding.weight) Parameter containing: tensor([[ 0.1744, 1.3013, -0.9791, ..., -0.0872, 0.4686, -0.9148], [-0.5932, 0.0042, -0.0580, ..., -1.7171, 2.0935, -1.3774], [-0.6436, -0.4488, 2.2102, ..., -0.2626, -0.0759, 0.7769], ..., [-0.0236, -1.0380, 1.0186, ..., -1.6911, 0.4438, -0.1033], [ 0.3624, -0.3315, -0.2723, ..., 0.8990, -0.5651, -0.2654], [ 0.3786, 0.9338, 0.7280, ..., -1.8523, -1.1715, -0.9778]], requires_grad=True) &gt;&gt;&gt; w = embedding.weight.reshape(1,-1).tolist()[0] &gt;&gt;&gt; stats.normaltest(w) NormaltestResult(statistic=0.2605386827757071, pvalue=0.8778589553262913) We can also plot those values by below code to help check it. &gt;&gt;&gt; s = pd.DataFrame(w,columns = ['value']) &gt;&gt;&gt; &gt;&gt;&gt; fig = plt.figure(figsize = (10,6)) &gt;&gt;&gt; ax1 = fig.add_subplot(2,1,1) &gt;&gt;&gt; ax1.scatter(s.index, s.values) &lt;matplotlib.collections.PathCollection object at 0x000001B56A6985E0&gt; &gt;&gt;&gt; plt.grid() &gt;&gt;&gt; &gt;&gt;&gt; ax2 = fig.add_subplot(2,1,2) &gt;&gt;&gt; s.hist(bins=30,alpha = 0.5,ax = ax2) array([&lt;AxesSubplot:title={'center':'value'}&gt;], dtype=object) &gt;&gt;&gt; s.plot(kind = 'kde', secondary_y=True,ax = ax2) &lt;AxesSubplot:&gt; &gt;&gt;&gt; plt.grid() &gt;&gt;&gt; plt.show() After the embedding object is created, we can parsing some tensors with integers directly to the object and it will return the tensors with length embedding_dim from the embedding.weight according to those integers as indices of the tensors. As below instance shows, the embedding is a 5x3 tensor whose values are normally distributed, and the input is a 2x4 tensor whose values are from [0, 5). The first dimension of the output tensor after embedding is a 2 which is equal to the first dimension of the input. The second dimension of the output is 4 since each tensor in the input contains 4 integers and each integer is treated as an index in embedding.weight. So the first tensor [0, 2, 0, 1] in the input means just copying of the 0th, 2th, 0th, and 1th tensor from the embedding tensor to the output and there is why the third dimension of the output is 3 because it just copies tensors from embedding without changing their shapes. &gt;&gt;&gt; import torch &gt;&gt;&gt; from torch import nn &gt;&gt;&gt; from scipy import stats &gt;&gt;&gt; &gt;&gt;&gt; embedding = nn.Embedding(5,3) &gt;&gt;&gt; print(embedding.weight) Parameter containing: tensor([[-1.5359, 1.3167, -0.4135], [-0.1170, 0.9554, -0.7263], [-0.3082, -1.0919, -1.2622], [ 0.3853, -1.9481, -0.1821], [ 1.0909, -1.1010, 0.6301]], requires_grad=True) &gt;&gt;&gt; input = torch.LongTensor([[0, 2, 0, 1], [1, 3, 4, 4]]) &gt;&gt;&gt; print(input.size()) torch.Size([2, 4]) &gt;&gt;&gt; output = embedding(input) &gt;&gt;&gt; print(output.size()) torch.Size([2, 4, 3]) &gt;&gt;&gt; print(output) tensor([[[-1.5359, 1.3167, -0.4135], [-0.3082, -1.0919, -1.2622], [-1.5359, 1.3167, -0.4135], [-0.1170, 0.9554, -0.7263]], [[-0.1170, 0.9554, -0.7263], [ 0.3853, -1.9481, -0.1821], [ 1.0909, -1.1010, 0.6301], [ 1.0909, -1.1010, 0.6301]]], grad_fn=&lt;EmbeddingBackward0&gt;) &gt;&gt;&gt; Therefore, after knowing how nn.embedding works, we can use it to embed words. As below sample code shows, because the word dictionary only has 2 words, we only need 2 embedding tensors and the embedding dimension is set to 5 because we want each word will be tranferred to a 5 dimensional tensor. We can set the embedding dimension to any other positive integers greater than 0. So according to the example, the word hello is embedded as [ 1.5414, -0.8476, 1.2966, -1.1901, -0.0852] which is the 0th tensor of embeds.weight, because the value of the word is 0 in the word_to_ix dictionary. &gt;&gt;&gt; import torch &gt;&gt;&gt; import torch.nn as nn &gt;&gt;&gt; from torch.autograd import Variable &gt;&gt;&gt; &gt;&gt;&gt; word_to_ix = {'hello': 0, 'world': 1} &gt;&gt;&gt; embeds = nn.Embedding(2, 5) &gt;&gt;&gt; print(embeds.weight) Parameter containing: tensor([[ 1.5414, -0.8476, 1.2966, -1.1901, -0.0852], [-1.9759, -0.4362, -0.9985, 1.3360, 0.1116]], requires_grad=True) &gt;&gt;&gt; &gt;&gt;&gt; hello_idx = torch.LongTensor([word_to_ix['hello']]) &gt;&gt;&gt; hello_idx = Variable(hello_idx) &gt;&gt;&gt; hello_embed = embeds(hello_idx) &gt;&gt;&gt; print(hello_embed) tensor([[ 1.5414, -0.8476, 1.2966, -1.1901, -0.0852]], grad_fn=&lt;EmbeddingBackward0&gt;) &gt;&gt;&gt; Backing to the source code, now we know that the self.embedding() in the forward function will embed tensors with indices to other tensors. The tokens.long() converts the values in the tensor to integers similar to math.floor(). The reason of multiplying the embedded tensor by math.sqrt(self.emb_size) is in section 3.4 of the original paper. The details are related to another paper, so far, I don't get it. ","link":"https://edwardxliu.github.io/post/code-review-of-language-translation-with-nntransformer-and-torchtext-part-2/"},{"title":"OLD TIME","content":"That year, breeze kissed trees Fallen leaves swayed as they were sleeping That year, your smile was sweet flickering across your lips Dream shines in eyes Memory is following the familiar melody fading away gradually Back to the past Miss you all night Dream of the past Does it come true at present Oh the old times have disappeared in a blink of eyes I haven't told you that there's love everlast Today I live very well Walking through the familiar streets where strangers nod and smile but where have you been all the while Softly hum the songs Tear is fermented by the past Stories have gone with the dispersed crowd but I've been keeping them in heart Back to the past Miss you all night Dream of the past Does it come true at present Oh the old times have disappeared in a blink of eyes I haven't told you that there's love everlast ","link":"https://edwardxliu.github.io/post/old-time/"},{"title":"CODE REVIEW OF LANGUAGE TRANSLATION WITH NN.TRANSFORMER AND TORCHTEXT - PART 1","content":"Data Sourcing and Processing The first part of the source code related to data processing is showed below. from torchtext.data.utils import get_tokenizer from torchtext.vocab import build_vocab_from_iterator from torchtext.datasets import Multi30k from typing import Iterable, List SRC_LANGUAGE = 'de' TGT_LANGUAGE = 'en' # Place-holders token_transform = {} vocab_transform = {} # Create source and target language tokenizer. Make sure to install the dependencies. # pip install -U spacy # python -m spacy download en_core_web_sm # python -m spacy download de_core_news_sm token_transform[SRC_LANGUAGE] = get_tokenizer('spacy', language='de_core_news_sm') token_transform[TGT_LANGUAGE] = get_tokenizer('spacy', language='en_core_web_sm') token_transform and vocab_transform are two dictionaries for storing the token transfomers of the source (German) and target (English) languages. The token transfomer get_tokenizer is a word-segmentation function for transferring a string sentence into a list of words and punctuation characters. The below piece of code shows an example of splitting a English sentence into a list of English words by spaces with the basic_english tokenizer. &gt;&gt;&gt; import torchtext &gt;&gt;&gt; from torchtext.data import get_tokenizer &gt;&gt;&gt; tokenizer = get_tokenizer(&quot;basic_english&quot;) &gt;&gt;&gt; tokens = tokenizer(&quot;You can now install TorchText using pip!&quot;) &gt;&gt;&gt; tokens &gt;&gt;&gt; ['you', 'can', 'now', 'install', 'torchtext', 'using', 'pip', '!'] Like the code presented before, the get_tokenizer function can also call other tokenizer libraries (e.g. spacy, moses, toktok, revtok, subword) if they are preinstalled. It should be noticed that the tokenizer from spacy performs better than the original basic_english. For example, we can do a comparison test like below showed. &gt;&gt;&gt; get_tokenizer(&quot;basic_english&quot;)(&quot;Let's go to N.Y.!&quot;) &gt;&gt;&gt; ['let', &quot;'&quot;, 's', 'go', 'to', 'n', '.', 'y', '.', '!'] &gt;&gt;&gt; get_tokenizer(&quot;spacy&quot;, language=&quot;en_core_web_sm&quot;)(&quot;Let's go to N.Y.!&quot;) &gt;&gt;&gt; ['Let', &quot;'s&quot;, 'go', 'to', 'N.Y.', '!'] Moreover, I've also tested the performance of the spacy tokenizer on Chinese, but the result showed below is worse than expected because 清华大学 should not be segmented. &gt;&gt;&gt; get_tokenizer(&quot;spacy&quot;, language=&quot;zh_core_web_sm&quot;)(&quot;我在清华大学读书。&quot;) &gt;&gt;&gt; ['我', '在', '清华', '大学', '读书', '。'] The details about the spacy tokenization Tokenization is the task of splitting a text into meaningful segments, called tokens. The input to the tokenizer is a unicode text, and the output is a Doc object. To construct a Doc object, you need a Vocab instance, a sequence of word strings, and optionally a sequence of spaces booleans, which allow you to maintain alignment of the tokens into the original string. During processing, spaCy first tokenizes the text, i.e. segments it into words, punctuation and so on. This is done by applying rules specific to each language. For example, punctuation at the end of a sentence should be split off – whereas “U.K.” should remain one token. Each Doc consists of individual tokens, and we can iterate over them: &gt;&gt;&gt; import spacy &gt;&gt;&gt; nlp = spacy.load(&quot;en_core_web_sm&quot;) &gt;&gt;&gt; doc = nlp(&quot;Apple is looking at buying U.K. startup for $1 billion&quot;) &gt;&gt;&gt; for token in doc: &gt;&gt;&gt; print(token.text) 0 1 2 3 4 5 6 7 8 9 10 Apple is looking at buying U.K. startup for $ 1 billion First, the raw text is split on whitespace characters, similar to text.split(' '). Then, the tokenizer processes the text from left to right. On each substring, it performs two checks: Does the substring match a tokenizer exception rule? For example, “don’t” does not contain whitespace, but should be split into two tokens, “do” and “n’t”, while “U.K.” should always remain one token. Can a prefix, suffix or infix be split off? For example punctuation like commas, periods, hyphens or quotes. If there’s a match, the rule is applied and the tokenizer continues its loop, starting with the newly split substrings. This way, spaCy can split complex, nested tokens like combinations of abbreviations and multiple punctuation marks. While punctuation rules are usually pretty general, tokenizer exceptions strongly depend on the specifics of the individual language. This is why each available language has its own subclass, like English or German, that loads in lists of hard-coded data and exception rules. Algorithm details: How spaCy's tokenizer works spaCy introduces a novel tokenization algorithm that gives a better balance between performance, ease of definition and ease of alignment into the original string. After consuming a prefix or suffix, we consult the special cases again. We want the special cases to handle things like “don’t” in English, and we want the same rule to work for “(don’t)!“. We do this by splitting off the open bracket, then the exclamation, then the closed bracket, and finally matching the special case. Here’s an implementation of the algorithm in Python optimized for readability rather than performance: def tokenizer_pseudo_code( text, special_cases, prefix_search, suffix_search, infix_finditer, token_match, url_match ): tokens = [] for substring in text.split(): suffixes = [] while substring: while prefix_search(substring) or suffix_search(substring): if token_match(substring): tokens.append(substring) substring = &quot;&quot; break if substring in special_cases: tokens.extend(special_cases[substring]) substring = &quot;&quot; break if prefix_search(substring): split = prefix_search(substring).end() tokens.append(substring[:split]) substring = substring[split:] if substring in special_cases: continue if suffix_search(substring): split = suffix_search(substring).start() suffixes.append(substring[split:]) substring = substring[:split] if token_match(substring): tokens.append(substring) substring = &quot;&quot; elif url_match(substring): tokens.append(substring) substring = &quot;&quot; elif substring in special_cases: tokens.extend(special_cases[substring]) substring = &quot;&quot; elif list(infix_finditer(substring)): infixes = infix_finditer(substring) offset = 0 for match in infixes: if offset == 0 and match.start() == 0: continue tokens.append(substring[offset : match.start()]) tokens.append(substring[match.start() : match.end()]) offset = match.end() if substring[offset:]: tokens.append(substring[offset:]) substring = &quot;&quot; elif substring: tokens.append(substring) substring = &quot;&quot; tokens.extend(reversed(suffixes)) for match in matcher(special_cases, text): tokens.replace(match, special_cases[match]) return tokens The algorithm can be summarized as follows: Iterate over space-separated substrings. Look for a token match. If there is a match, stop processing and keep this token. Check whether we have an explicitly defined special case for this substring. If we do, use it. Otherwise, try to consume one prefix. If we consumed a prefix, go back to #2, so that the token match and special cases always get priority. If we didn’t consume a prefix, try to consume a suffix and then go back to #2. If we can’t consume a prefix or a suffix, look for a URL match. If there’s no URL match, then look for a special case. Look for “infixes” – stuff like hyphens etc. and split the substring into tokens on all infixes. Once we can’t consume any more of the string, handle it as a single token. Make a final pass over the text to check for special cases that include spaces or that were missed due to the incremental processing of affixes. Global and language-specific tokenizer data is supplied via the language data in spacy/lang. The tokenizer exceptions define special cases like “don’t” in English, which needs to be split into two tokens: {ORTH: &quot;do&quot;} and {ORTH: &quot;n't&quot;, NORM: &quot;not&quot;}. The prefixes, suffixes and infixes mostly define punctuation rules – for example, when to split off periods (at the end of a sentence), and when to leave tokens containing periods intact (abbreviations like “U.S.”). The next part of the data processing code is basically a function for returning the list of tokens of a given sentence. # helper function to yield list of tokens def yield_tokens(data_iter: Iterable, language: str) -&gt; List[str]: language_index = {SRC_LANGUAGE: 0, TGT_LANGUAGE: 1} for data_sample in data_iter: yield token_transform[language](data_sample[language_index[language]]) Since the input dataset Multi30k is composed with a series of tuples where each tuple contains two strings (one is in German and the other one is the corresponding translation in English, and the order of them can be switched via the parameter language_pair), the language_index is used to determine the indices of the elements in a tuple. So if the input parameter language = 'de', the language_index[language] will return 0, and it will return 1 if language = 'en' because of language_index = {SRC_LANGUAGE: 0, TGT_LANGUAGE: 1}. Therefore, data_sample[language_index[language]] in the for loop will traverse each tuple in the given dataset and be like data_sample[0] or data_sample[1] depending on the value of language, then returns the corresponding string sentences. After that, token_transform[language] will call corresponding tokenizer and transfer the sentence to a list of tokens. The following code may help you understand the yield_tokens function. &gt;&gt;&gt; [i for i in Multi30k(split='valid', language_pair=('de', 'en'))][0] &gt;&gt;&gt; ('Eine Gruppe von Männern lädt Baumwolle auf einen Lastwagen', 'A group of men are loading cotton onto a truck') &gt;&gt;&gt; [i for i in Multi30k(split='valid', language_pair=('en', 'de'))][0] &gt;&gt;&gt; ('A group of men are loading cotton onto a truck', 'Eine Gruppe von Männern lädt Baumwolle auf einen Lastwagen') &gt;&gt;&gt; &gt;&gt;&gt; for data_sample in Multi30k(split='valid', language_pair=('de', 'en')): ... print(data_sample[0], token_transform['de'](data_sample[0])) ... print(data_sample[1], token_transform['en'](data_sample[1])) ... break ... &gt;&gt;&gt; Eine Gruppe von Männern lädt Baumwolle auf einen Lastwagen ['Eine', 'Gruppe', 'von', 'Männern', 'lädt', 'Baumwolle', 'auf', 'einen', 'Lastwagen'] &gt;&gt;&gt; A group of men are loading cotton onto a truck ['A', 'group', 'of', 'men', 'are', 'loading', 'cotton', 'onto', 'a', 'truck'] The next part of the code is used to build two word dictionaries (German and English) in which each word is assigned with a unique integer (starting from 0 to the number of unique words in the given dataset). The words and their values will be used to build word vectors later. # Define special symbols and indices UNK_IDX, PAD_IDX, BOS_IDX, EOS_IDX = 0, 1, 2, 3 # Make sure the tokens are in order of their indices to properly insert them in vocab special_symbols = ['&lt;unk&gt;', '&lt;pad&gt;', '&lt;bos&gt;', '&lt;eos&gt;'] for ln in [SRC_LANGUAGE, TGT_LANGUAGE]: # Training data Iterator train_iter = Multi30k(split='train', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE)) # Create torchtext's Vocab object vocab_transform[ln] = build_vocab_from_iterator(yield_tokens(train_iter, ln), min_freq=1, specials=special_symbols, special_first=True) # Set UNK_IDX as the default index. This index is returned when the token is not found. # If not set, it throws RuntimeError when the queried token is not found in the Vocabulary. for ln in [SRC_LANGUAGE, TGT_LANGUAGE]: vocab_transform[ln].set_default_index(UNK_IDX) The build_vocab_from_iterator function will return a Vocab which is an object from torchtext. We can use get_stoi() function to list all the keys and values in that object. Noticing that specials and special_first are set, which means the values of four special_symbols are set from 0 to 3 in the dictionary. We can also play around with the Vocab and do some checks related to the keys and values. &gt;&gt;&gt; vocab_transform['en'].get_stoi() &gt;&gt;&gt; {'pitch': 1533, 'pouring': 1021, 'point': 2242, 'wires': 1869, 'fruit': 529, 'Some': 431, '.': 5, 'advice': 7488, 'audio': 4055, 'park': 120, '&lt;bos&gt;': 2, ... } &gt;&gt;&gt; &gt;&gt;&gt; all_unique_words_in_training_data = set([ j for i in yield_tokens(train_iter, 'en') for j in i ]) &gt;&gt;&gt; all_unique_words_in_vocab = set(vocab_transform['en'].get_itos()) &gt;&gt;&gt; all_unique_words_in_vocab.difference(all_unique_words_in_training_data) &gt;&gt;&gt; {'&lt;unk&gt;', '&lt;bos&gt;', '&lt;eos&gt;', '&lt;pad&gt;'} &gt;&gt;&gt; &gt;&gt;&gt; print(len(vocab_transform['en'].get_stoi())) &gt;&gt;&gt; 10837 &gt;&gt;&gt; d = vocab_transform['en'].get_stoi() &gt;&gt;&gt; print(min(d.values()), max(d.values())) &gt;&gt;&gt; 0 10836 In addition, the assigned value of each word in vocab_transform is related to the decreasing order of the occurrence frequency of the word in the dataset. We can verify this by below code. Noticing that the first 4 elements of vocab_transform are special symbols, so the value of the most frequently occurring word &quot;a&quot; is set to be 4, followed by words &quot;.&quot;, &quot;A&quot;, &quot;in&quot;, and &quot;the&quot; with the assigned values from 5 to 8 relatively. &gt;&gt;&gt; import pandas as pd &gt;&gt;&gt; &gt;&gt;&gt; all_words_in_training_data = [ j for i in yield_tokens(train_iter, 'en') for j in i ] &gt;&gt;&gt; df = pd.DataFrame(all_words_in_training_data, columns=['word']) &gt;&gt;&gt; word_freq = df.groupby([&quot;word&quot;])[&quot;word&quot;].count().reset_index(name=&quot;count&quot;).sort_values(by=[&quot;count&quot;], ascending=False) &gt;&gt;&gt; print(word_freq.head(5)) word count 1875 a 31707 16 . 27623 97 A 17458 5816 in 14847 9859 the 9923 &gt;&gt;&gt; [ vocab_transform['en'].get_stoi()[i] for i in word_freq.head(5)['word'].tolist()] [4, 5, 6, 7, 8] ","link":"https://edwardxliu.github.io/post/code-review-of-language-translation-with-nntransformer-and-torchtext-part-1/"},{"title":"Test","content":"this is for test usage LaTeX 公式 可以创建行内公式，例如 Γ(n)=(n−1)!∀n∈N\\Gamma(n) = (n-1)!\\quad\\forall n\\in\\mathbb NΓ(n)=(n−1)!∀n∈N。或者块级公式： x=−b±b2−4ac2ax = \\dfrac{-b \\pm \\sqrt{b^2 - 4ac}}{2a} x=2a−b±b2−4ac​​ pu=PuTνuU\\pmb p_u=\\pmb P^T_uν^U_u p​p​​pu​=PPPuT​νuU​ \\begin{equation}\\label{eq:mti} \\begin{split} Ttran_{i}{j,j+1}=\\frac{R_{bd_{j,j+1}}{ij}}{AR_{bd_{j,j+1}}^{ij}}\\times T_{ideal_tran}^{ij},\\ v,u \\in \\left [ 1,|E_{S}| \\right ],\\ j,j+1 \\in \\left [ 1,|sfc_{i}| \\right ],i \\in \\left [ 1,|SC| \\right ]. \\end{split} \\end{equation} 表格 Item Value Qty Computer 1600 USD 5 Phone 12 USD 12 Pipe 1 USD 234 ","link":"https://edwardxliu.github.io/post/test/"},{"title":"关于","content":" 欢迎来到我的小站呀，很高兴遇见你！🤝 🏠 关于本站 As the old saying goes, history is the great teacher, we will all someday be teachers ourselves because someday we will all be history too. We will someday be the ancients, and we can choose what that will mean 👨‍💻 博主是谁 EDWARD LIU/男/1989 硕士/悉尼大学/计算机科学 技术博客: https://edwardxliu.github.io/ Github: https://github.com/edwardxliu ⛹ 兴趣爱好 神神叨叨 📬 联系我呀 Email: edward.ed.liu@gmail.com ","link":"https://edwardxliu.github.io/post/guan-yu/"}]}