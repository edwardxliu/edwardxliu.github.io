<!DOCTYPE html>
<html>

<head>
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta content="yes" name="apple-mobile-web-app-capable" />
<meta content="black" name="apple-mobile-web-app-status-bar-style" />
<meta name="referrer" content="never">
<meta name="keywords" content="">
<meta name="description" content="欢迎访问[Edward&#39;s Blog]的个人博客">
<meta name="author" content="kveln">
<title>CODE REVIEW OF LANGUAGE TRANSLATION WITH NN.TRANSFORMER AND TORCHTEXT - PART 2 | Edward&#39;s Blog</title>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.5.0/dist/css/bootstrap.min.css">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.10.0/katex.min.css">
<link href="https://cdn.bootcss.com/font-awesome/5.11.2/css/all.min.css" rel="stylesheet">
<link href='https://fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
<link
  href='https://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800'
  rel='stylesheet' type='text/css'>
<link rel="alternate" type="application/rss+xml" title="CODE REVIEW OF LANGUAGE TRANSLATION WITH NN.TRANSFORMER AND TORCHTEXT - PART 2 | Edward&#39;s Blog » Feed"
  href="https://edwardxliu.github.io/atom.xml">
<link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@9.15.10/build/styles/androidstudio.min.css">
<link href="https://edwardxliu.github.io/styles/main.css" rel="stylesheet">
<script src="https://cdn.bootcss.com/jquery/3.4.1/jquery.min.js"></script>

<script src="https://cdn.jsdelivr.net/npm/@highlightjs/cdn-assets/highlight.min.js"></script>

<link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/850552586/ericamcdn@0.1/css/live2d.css">

<script>hljs.initHighlightingOnLoad();</script>

  <meta property="og:description" content="CODE REVIEW OF LANGUAGE TRANSLATION WITH NN.TRANSFORMER AND TORCHTEXT - PART 2" />
  <meta property="og:url" content="https://edwardxliu.github.io/post/code-review-of-language-translation-with-nntransformer-and-torchtext-part-2/" />
  <meta property="og:locale" content="zh-CN" />
  <meta property="og:type" content="website" />
  <meta property="og:site_name" content="Edward&#39;s Blog" />
  <!-- <script src="../assets/styles/scripts/tocScript.js"></script> -->
</head>

<body>
  <!-- Navigation -->
  <nav class="navbar navbar-expand-lg navbar-light fixed-top" id="mainNav">
  <div class="container">
    <a class="navbar-brand" href="https://edwardxliu.github.io">Edward&#39;s Blog</a>
    <button class="navbar-toggler navbar-toggler-right" type="button" data-toggle="collapse"
      data-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false"
      aria-label="Toggle navigation">
      Menu
      <i class="fas fa-bars"></i>
    </button>
    <div class="collapse navbar-collapse" id="navbarResponsive">
      <ul class="navbar-nav ml-auto">
        
        <li class="nav-item">
          
          <a class="nav-link" href="/">首页</a>
          
        </li>
        
        <li class="nav-item">
          
          <a class="nav-link" href="/archives">归档</a>
          
        </li>
        
        <li class="nav-item">
          
          <a class="nav-link" href="/tags">标签</a>
          
        </li>
        
        <li class="nav-item">
          
          <a class="nav-link" href="https://edwardxliu.github.io/post/guan-yu">关于</a>
          
        </li>
        
        <li class="nav-item">
          <div class="gridea-search-container">
            <form id="gridea-search-form" style="position: relative" data-update="1659681269797"
              action="/search/index.html">
              <input class="search-input" autocomplete="off" spellcheck="false" name="q" placeholder="搜索文章" />
              <i class="fas fa-search gt-c-content-color-first" style="position: absolute; top: 9px; left: 10px;"></i>
            </form>
          </div>
        </li>
      </ul>
    </div>
  </div>
</nav>
  <!-- Page Header -->
  <nav class="navbar navbar-expand-lg navbar-light fixed-top" id="mainNav">
  <div class="container">
    <a class="navbar-brand" href="https://edwardxliu.github.io">Edward&#39;s Blog</a>
    <button class="navbar-toggler navbar-toggler-right" type="button" data-toggle="collapse"
      data-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false"
      aria-label="Toggle navigation">
      Menu
      <i class="fas fa-bars"></i>
    </button>
    <div class="collapse navbar-collapse" id="navbarResponsive">
      <ul class="navbar-nav ml-auto">
        
        <li class="nav-item">
          
          <a class="nav-link" href="/">首页</a>
          
        </li>
        
        <li class="nav-item">
          
          <a class="nav-link" href="/archives">归档</a>
          
        </li>
        
        <li class="nav-item">
          
          <a class="nav-link" href="/tags">标签</a>
          
        </li>
        
        <li class="nav-item">
          
          <a class="nav-link" href="https://edwardxliu.github.io/post/guan-yu">关于</a>
          
        </li>
        
        <li class="nav-item">
          <div class="gridea-search-container">
            <form id="gridea-search-form" style="position: relative" data-update="1659681269797"
              action="/search/index.html">
              <input class="search-input" autocomplete="off" spellcheck="false" name="q" placeholder="搜索文章" />
              <i class="fas fa-search gt-c-content-color-first" style="position: absolute; top: 9px; left: 10px;"></i>
            </form>
          </div>
        </li>
      </ul>
    </div>
  </div>
</nav>
<header class="masthead" style="background-image: url('https://edwardxliu.github.io/media/images/home-bg.jpg')">
  <div class="overlay"></div>
  <div class="container">
    <div class="row">
      <div class="col-lg-8 col-md-10 mx-auto">
        
          <!-- 没Title为其他页面Header -->
          
            <!-- 没Title并且有headerType为Post：文章Header -->
            <div class="post-heading">
              <span class="tags">
                
              </span>
              <h1>CODE REVIEW OF LANGUAGE TRANSLATION WITH NN.TRANSFORMER AND TORCHTEXT - PART 2</h1>
              <span class="meta">
                Posted on
                2022-04-02，8 min read
              </span>
            </div>
          
        
      </div>
    </div>
  </div>
</header>
  <!-- Post Content -->
  <article id="post-content-article">
    <div class="container">
      <div class="row">
        <div class="col-lg-8 col-md-10 mx-auto post-content-container">
          
          <img class="post-feature-header-image" src="https://raw.githubusercontent.com/edwardxliu/Picbed_PicGo/main/nature88885_275864358_676329820155573_4789674992585309938_n.jpg" alt="封面图">
          </img>
          
          <h1 id="token-embedding">Token Embedding</h1>
<p>The second part of the <a href="https://pytorch.org/tutorials/beginner/translation_transformer.html">source code</a> is related to token and its position embedding. We reivew the former firstly.</p>
<pre><code class="language-python">from torch import Tensor
import torch
import torch.nn as nn
from torch.nn import Transformer
import math
DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# helper Module to convert tensor of input indices into corresponding tensor of token embeddings
class TokenEmbedding(nn.Module):
    def __init__(self, vocab_size: int, emb_size):
        super(TokenEmbedding, self).__init__()
        self.embedding = nn.Embedding(vocab_size, emb_size)
        self.emb_size = emb_size

    def forward(self, tokens: Tensor):
        return self.embedding(tokens.long()) * math.sqrt(self.emb_size)
</code></pre>
<p>The <code>TokenEmbedding</code> class just includes an initialize function and a forward function where the <code>__init__</code> function (aka. the constructor) is called when the class is instantiated as an object, and the <code>forward</code> function is called when parsing parameters directly to the object, which invokes the <code>__call__</code> method because the class simply inherits the <code>__call__</code> method of the <code>nn.Module</code> class. It is here where the <code>forward</code> method is called. Below test code presents an example.</p>
<pre><code class="language-python">&gt;&gt;&gt; class Test(nn.Module):
...     def __init__(self):
...         super(Test, self).__init__()
...         print('Test class __init__')
...     def forward(self, x):
...         return x + 1
...
&gt;&gt;&gt; t = Test()
Test class __init__
&gt;&gt;&gt; t(1)
2
&gt;&gt;&gt;
</code></pre>
<p>Back to the source code, the initialization of the <code>TokenEmbedding</code> class is related to the instantiation of <code>nn.Embedding</code> class and two parameters, <code>vocab_size</code> and <code>emb_size</code> which refer to the length of the token dictionary <code>vocab_transform</code> mentioned in <a href="https://edwardxliu.github.io/post/code-review-of-language-translation-with-nntransformer-and-torchtext-part-1/">here</a> and the length of each word vector. So, each token in the dictionary will be associated with a <strong>number vector (tensor)</strong> of length equal to <code>emb_size</code>.</p>
<p>The <code>nn.embedding(num_embeddings, embedding_dim)</code> will initialize a tensor (learnable weights) with shape (num_embeddings, embedding_dim) from <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi mathvariant="double-struck">N</mi><mrow><mo fence="true">(</mo><mn>0</mn><mo separator="true">,</mo><mn>1</mn><mo fence="true">)</mo></mrow></mrow><annotation encoding="application/x-tex">\mathbb{N}\left ( 0, 1 \right )</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathbb">N</span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;">(</span><span class="mord">0</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord">1</span><span class="mclose delimcenter" style="top:0em;">)</span></span></span></span></span>. We can review the values in the tensor by calling the <code>.weight</code> function and check if it is normally distributed by <strong>p value</strong> from <code>scipy.stats.normaltest</code> function as below code shows. It is more likely to be a normal distribution when p value is closer to 1.</p>
<pre><code class="language-python">&gt;&gt;&gt; from torch import nn
&gt;&gt;&gt; from scipy import stats
&gt;&gt;&gt;
&gt;&gt;&gt; embedding = nn.Embedding(10,512)
&gt;&gt;&gt; print(embedding.weight.shape)
torch.Size([10, 512])
&gt;&gt;&gt; print(embedding.weight)
Parameter containing:
tensor([[ 0.1744,  1.3013, -0.9791,  ..., -0.0872,  0.4686, -0.9148],
        [-0.5932,  0.0042, -0.0580,  ..., -1.7171,  2.0935, -1.3774],
        [-0.6436, -0.4488,  2.2102,  ..., -0.2626, -0.0759,  0.7769],
        ...,
        [-0.0236, -1.0380,  1.0186,  ..., -1.6911,  0.4438, -0.1033],
        [ 0.3624, -0.3315, -0.2723,  ...,  0.8990, -0.5651, -0.2654],
        [ 0.3786,  0.9338,  0.7280,  ..., -1.8523, -1.1715, -0.9778]],
       requires_grad=True)
&gt;&gt;&gt; w = embedding.weight.reshape(1,-1).tolist()[0]
&gt;&gt;&gt; stats.normaltest(w)
NormaltestResult(statistic=0.2605386827757071, pvalue=0.8778589553262913)
</code></pre>
<p>We can also plot those values by below code to help check it.</p>
<pre><code class="language-python">&gt;&gt;&gt; s = pd.DataFrame(w,columns = ['value'])
&gt;&gt;&gt;
&gt;&gt;&gt; fig = plt.figure(figsize = (10,6))
&gt;&gt;&gt; ax1 = fig.add_subplot(2,1,1)
&gt;&gt;&gt; ax1.scatter(s.index, s.values)
&lt;matplotlib.collections.PathCollection object at 0x000001B56A6985E0&gt;
&gt;&gt;&gt; plt.grid()
&gt;&gt;&gt;
&gt;&gt;&gt; ax2 = fig.add_subplot(2,1,2)
&gt;&gt;&gt; s.hist(bins=30,alpha = 0.5,ax = ax2)
array([&lt;AxesSubplot:title={'center':'value'}&gt;], dtype=object)
&gt;&gt;&gt; s.plot(kind = 'kde', secondary_y=True,ax = ax2)
&lt;AxesSubplot:&gt;
&gt;&gt;&gt; plt.grid()
&gt;&gt;&gt; plt.show()
</code></pre>
<p><img src="https://raw.githubusercontent.com/edwardxliu/Picbed_PicGo/main/Figure_1.png#pic_center" alt="plot result img" loading="lazy"><br>
After the <code>embedding</code> object is created, we can parsing some tensors with integers directly to the object and it will return the tensors with length <code>embedding_dim</code> from the <code>embedding.weight</code> according to those integers as indices of the tensors. As below instance shows, the embedding is a 5x3 tensor whose values are normally distributed, and the input is a 2x4 tensor whose values are from [0, 5). The first dimension of the output tensor after embedding is a 2 which is equal to the first dimension of the input. The second dimension of the output is 4 since each tensor in the input contains 4 integers and each integer is treated as an index in <code>embedding.weight</code>. So the first tensor <strong>[0, 2, 0, 1]</strong> in the input means just copying of the <strong>0th, 2th, 0th, and 1th</strong> tensor from the embedding tensor to the output and there is why the third dimension of the output is 3 because it just copies tensors from <code>embedding</code> without changing their shapes.</p>
<pre><code class="language-python">&gt;&gt;&gt; import torch
&gt;&gt;&gt; from torch import nn
&gt;&gt;&gt; from scipy import stats
&gt;&gt;&gt;
&gt;&gt;&gt; embedding = nn.Embedding(5,3)
&gt;&gt;&gt; print(embedding.weight)
Parameter containing:
tensor([[-1.5359,  1.3167, -0.4135],
        [-0.1170,  0.9554, -0.7263],
        [-0.3082, -1.0919, -1.2622],
        [ 0.3853, -1.9481, -0.1821],
        [ 1.0909, -1.1010,  0.6301]], requires_grad=True)
&gt;&gt;&gt; input = torch.LongTensor([[0, 2, 0, 1], [1, 3, 4, 4]])
&gt;&gt;&gt; print(input.size())
torch.Size([2, 4])
&gt;&gt;&gt; output = embedding(input)
&gt;&gt;&gt; print(output.size())
torch.Size([2, 4, 3])
&gt;&gt;&gt; print(output)
tensor([[[-1.5359,  1.3167, -0.4135],
         [-0.3082, -1.0919, -1.2622],
         [-1.5359,  1.3167, -0.4135],
         [-0.1170,  0.9554, -0.7263]],

        [[-0.1170,  0.9554, -0.7263],
         [ 0.3853, -1.9481, -0.1821],
         [ 1.0909, -1.1010,  0.6301],
         [ 1.0909, -1.1010,  0.6301]]], grad_fn=&lt;EmbeddingBackward0&gt;)
&gt;&gt;&gt;         
</code></pre>
<p>Therefore, after knowing how <code>nn.embedding</code> works, we can use it to embed words. As below sample code shows, because the word dictionary only has 2 words, we only need 2 embedding tensors and the embedding dimension is set to 5 because we want each word will be tranferred to a 5 dimensional tensor. We can set the embedding dimension to any other positive integers greater than 0. So according to the example, the word <strong>hello</strong> is embedded as [ 1.5414, -0.8476,  1.2966, -1.1901, -0.0852] which is the 0th tensor of <code>embeds.weight</code>, because the value of the word is 0 in the <code>word_to_ix</code> dictionary.</p>
<pre><code class="language-python">&gt;&gt;&gt; import torch
&gt;&gt;&gt; import torch.nn as nn
&gt;&gt;&gt; from torch.autograd import Variable
&gt;&gt;&gt;
&gt;&gt;&gt; word_to_ix = {'hello': 0, 'world': 1}
&gt;&gt;&gt; embeds = nn.Embedding(2, 5)
&gt;&gt;&gt; print(embeds.weight)
Parameter containing:
tensor([[ 1.5414, -0.8476,  1.2966, -1.1901, -0.0852],
        [-1.9759, -0.4362, -0.9985,  1.3360,  0.1116]], requires_grad=True)
&gt;&gt;&gt;
&gt;&gt;&gt; hello_idx = torch.LongTensor([word_to_ix['hello']])
&gt;&gt;&gt; hello_idx = Variable(hello_idx)
&gt;&gt;&gt; hello_embed = embeds(hello_idx)
&gt;&gt;&gt; print(hello_embed)
tensor([[ 1.5414, -0.8476,  1.2966, -1.1901, -0.0852]],
       grad_fn=&lt;EmbeddingBackward0&gt;)
&gt;&gt;&gt;
</code></pre>
<p>Backing to the source code, now we know that the <code>self.embedding()</code> in the <code>forward</code> function will embed tensors with indices to other tensors. The <code>tokens.long()</code> converts the values in the tensor to integers similar to <code>math.floor()</code>. The reason of multiplying the embedded tensor by <code>math.sqrt(self.emb_size)</code> is in section 3.4 of the original <a href="https://arxiv.org/pdf/1706.03762.pdf">paper</a>. The details are related to another paper, so far, I don't get it.</p>

          <div class="toc-container"><ul class="markdownIt-TOC">
<li><a href="#token-embedding">Token Embedding</a></li>
</ul>
</div>
          
          <hr />
          <p class="next-post">下一篇：
            <a href="https://edwardxliu.github.io/post/old-time/">
              <span class="post-title">
                OLD TIME&rarr;
              </span>
            </a>
          </p>
          
          <div class="comment" style="text-align: center;">
            

            
            
            <link rel="stylesheet" href="https://unpkg.com/gitalk/dist/gitalk.css">
<script src="https://unpkg.com/gitalk/dist/gitalk.min.js"></script>
<div id="gitalk-container"></div>
<script>
  var gitalk = new Gitalk({
    clientID: 'f738c647c12a9501e1b8',
    clientSecret: '761bdc5f9bce460ba20b4faff8d769a6e3244d6d',
    repo: 'edwardxliu.github.io',
    owner: 'edwardxliu',
    admin: ['edwardxliu'],
    id: (location.pathname).substring(0, 49),      // Ensure uniqueness and length less than 50
    distractionFreeMode: false  // Facebook-like distraction free mode
  })
  gitalk.render('gitalk-container')
</script>

            
            
            
            
          </div>
        </div>
      </div>
  </article>
  <!-- Footer -->
  <footer>
    <div class="container">
      <div class="row">
        <div class="col-lg-8 col-md-10 mx-auto">
          <ul class="list-inline text-center">
            
            
              
            
              
            
              
            
              
            
              
            
              
            
              
              <!-- <li class="list-inline-item">
              <a href="https://edwardxliu.github.io/atom.xml" target="_blank">
                <span class="fa-stack fa-lg">
                  <i class="fas fa-circle fa-stack-2x"></i>
                  <i class="fas fa-rss fa-stack-1x fa-inverse"></i>
                </span>
              </a>
              </li> -->
          </ul>
          <p class="copyright text-muted">Copyright &copy;<span>Edward&#39;s Blog</span><br><a href="https://github.com/getgridea/gridea" class="Themeinfo">Powered by Gridea</a></p>
        </div>
      </div>
    </div>
   </footer>
  <!-- Bootstrap core JavaScript -->
  <script src="https://cdn.bootcss.com/twitter-bootstrap/4.3.1/js/bootstrap.bundle.min.js"></script>
  <!-- <script src="https://edwardxliu.github.io/media/scripts/bootstrap.bundle.min.js"></script> -->
  <!-- Bootstrap core JavaScript -->
  <script src="https://cdn.jsdelivr.net/gh/Alanrk/clean-cdn@1.0/scripts/clean-blog.min.js"></script>
  <!-- <script src="https://edwardxliu.github.io/media/scripts/clean-blog.min.js"></script> -->
  <script src="//instant.page/3.0.0" type="module" defer integrity="sha384-OeDn4XE77tdHo8pGtE1apMPmAipjoxUQ++eeJa6EtJCfHlvijigWiJpD7VDPWXV1"></script>
  <style type="text/css">a.back_to_top{text-decoration:none;position:fixed;bottom:40px;right:30px;background:#f0f0f0;height:40px;width:40px;border-radius:50%;line-height:36px;font-size:18px;text-align:center;transition-duration:.5s;transition-propety:background-color;display:none}a.back_to_top span{color:#888}a.back_to_top:hover{cursor:pointer;background:#dfdfdf}a.back_to_top:hover span{color:#555}@media print,screen and(max-width:580px){.back_to_top{display:none!important}}</style>
<a id="back_to_top" href="#" class="back_to_top">
  <span>▲</span></a>
<script>$(document).ready((function(_this) {
    return function() {
      var bt;
      bt = $('#back_to_top');
      if ($(document).width() > 480) {
        $(window).scroll(function() {
          var st;
          st = $(window).scrollTop();
          if (st > 30) {
            return bt.css('display', 'block')
          } else {
            return bt.css('display', 'none')
          }
        });
        return bt.click(function() {
          $('body,html').animate({
            scrollTop: 0
          },
          800);
          return false
        })
      }
    }
  })(this));</script>
  
  <div id="landlord-parent">
    <div id="landlord">
        <div class="message" style="opacity:0"></div>
        <canvas id="live2d" width="240" height="250" class="live2d"></canvas>
    </div>
</div>

<script type="text/javascript">
    if (/(iPhone|iPad|iPod|iOS|Android)/i.test(navigator.userAgent)) {
        //移动端
        console.log("------ 移动端");
    } else {
        console.log("------ PC端 " + navigator.userAgent);

        addScript("https://cdn.jsdelivr.net/gh/850552586/ericamcdn@0.1/js/live2d.js", () => {
            // 加载完成后再loadlive2d
            loadlive2d("live2d", "https://edwardxliu.github.io/media/live2d/assets/hijiki.model.json");
        });

        var home_Path = "https://edwardxliu.github.io/";
        addScript("https://edwardxliu.github.io/media/live2d/js/message.js", () => { });
    }

    // 插入js文件，完成后callback
    function addScript(jsfile, callback) {
        var landlord_parent = document.getElementById("landlord-parent");
        var script = document.createElement("script");
        script.type = "text/javascript";
        script.src = jsfile;
        landlord_parent.appendChild(script);
        script.onload = script.onreadystatechange = function () {
            if (!this.readyState || this.readyState === "loaded" || this.readyState === "complete") {
                script.onload = script.onreadystatechange = null;
                if (callback && typeof callback == "function") {
                    callback(); //window[callback]();如果传递字符串过来 调用window['函数名']() 调用方法
                }
            }
        };
    }
</script>
  
  <script src="https://edwardxliu.github.io/media/scripts/tocScript.js"></script>
</body>

</html>